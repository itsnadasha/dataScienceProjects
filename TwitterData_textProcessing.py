# -*- coding: utf-8 -*-
"""Copy of dataAnalysis2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10SDHkuJeYUsOfWI-G25DO0-rr47MptFv
"""

import numpy as np
import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler
import os # accessing directory structure

from google.colab import drive
drive.mount('/content/drive')

# Import Data
df = pd.read_csv('/content/drive/MyDrive/twcs.csv')
df.head()

import pandas as pd


# تحويل عمود التاريخ إلى تنسيق datetime
df['created_at'] = pd.to_datetime(df['created_at'])

# تعديل عمود التاريخ للاحتفاظ باليوم فقط
df['created_at'] = df['created_at'].dt.strftime('%a %b %d')
df.head(1000)

df['in_response_to_tweet_id'] = df['in_response_to_tweet_id'].fillna(0).replace([np.inf, -np.inf], 999)

# استبدال القيم النصية 'NaN' بقيمة NaN
df['in_response_to_tweet_id'] = df['in_response_to_tweet_id'].replace('NaN', np.nan)

# تحويل العمود إلى integer
df['in_response_to_tweet_id'] = df['in_response_to_tweet_id'].astype('int64')

print(df['in_response_to_tweet_id'].dtype)

import pandas as pd
import numpy as np

# Fill missing values with 0 and replace infinite values
df['response_tweet_id'] = df['response_tweet_id'].fillna(0).replace([np.inf, -np.inf], 999)

# Convert 'NaN' strings to actual NaN values (numpy.nan)
df['response_tweet_id'] = df['response_tweet_id'].replace('NaN', np.nan)

print(df['response_tweet_id'].dtype)

df.head(1000)



df.isnull().sum()

df.info()

df.duplicated().values.any()

# Remove duplicate rows based on all columns
df.drop_duplicates(inplace=True)

# Remove duplicates based on specific columns
# df.drop_duplicates(subset=['Name'], inplace=True)

print(df)

# Check for missing values
print(df.isnull().sum())

import pandas as pd
import re

# تعريف الدالة لإزالة الكلمات التي تبدأ بـ "@"
def remove_mentions(text):
    return re.sub(r'@\w+', '', text).strip()


df['text'] = df['text'].apply(remove_mentions)

import string # Import the string module to use string.punctuation

def remove_punct(text):
  text_nopunct = "".join([char for char in text if char not in string.punctuation])
  return text_nopunct

df['text'] = df['text'].apply(lambda x: remove_punct(x))
df.tail(10)

import nltk as nltk

from nltk.corpus import stopwords
nltk.download('stopwords') # Download the stopwords resource
stop_words = set(stopwords.words('english')) # Access the stopwords in lowercase

df.head(10)

# حفظ البيانات المعالجة إلى ملف CSV جديد بعد التنظيف
cleaned_file_path = 'cleaned_text.csv'
df.to_csv(cleaned_file_path, index=False)

print(f'تم حفظ البيانات المعالجة إلى: {cleaned_file_path}')